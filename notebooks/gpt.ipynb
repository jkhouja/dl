{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "74e03725-a411-48da-8d62-a64b256e7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import tqdm\n",
    "import math\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "474c0123-389e-4e73-92cb-f436be73c527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My version of losses\n",
    "\n",
    "\n",
    "# Naive implementation of softmax and cross entropy (numerically not stable - can underflow)\n",
    "def softmax(x):\n",
    "    # Operating on last dimension\n",
    "    x = torch.exp(x - torch.max(x, dim=-1, keepdim=True)[0])\n",
    "    x = x / x.sum(dim=-1, keepdim=True)\n",
    "    return x\n",
    "\n",
    "def cross_entropy(inp, targ, reduce=False):\n",
    "    # operating on last dimension and assuming 2D matrices\n",
    "    size, nclasses = inp.shape\n",
    "\n",
    "    # Create 1 hot encoding \n",
    "    #targets = torch.empty((size, nclasses))\n",
    "    #targets[torch.arange(size), targ] = 1\n",
    "\n",
    "    # easier way? maybe faster?\n",
    "    #targets.scatter_(-1, targets.view(-1,1), 1)\n",
    "\n",
    "    # calculate loss as the sum of log probabilities \n",
    "    l = -1.0 * torch.log_(inp[torch.arange(size), targ])\n",
    "\n",
    "    # targets.scatter_(-1, targets.view(-1,1), targets)\n",
    "\n",
    "    if reduce:\n",
    "        return l.mean()\n",
    "        \n",
    "    return l\n",
    "\n",
    "# Better implementaion (matches pytorch)\n",
    "def log_softmax(x):\n",
    "    # Operating on last dimension\n",
    "    x = x - torch.max(x, dim=-1, keepdim=True)[0] # shift by max\n",
    "    xe = torch.exp(x).sum(dim=-1, keepdim=True) # exp\n",
    "    return x - torch.log(xe)\n",
    "\n",
    "def cross_entropy_of_log(inp, targ, reduce=False):\n",
    "    # operating on last dimension and assuming 2D matrices\n",
    "    size, nclasses = inp.shape\n",
    "    l = -1.0 * inp[torch.arange(size), targ]\n",
    "\n",
    "    if reduce:\n",
    "        return l.mean()\n",
    "        \n",
    "    return l\n",
    "\n",
    "# Or combine both softmax and cross entropy\n",
    "def cross_entropy_logits(x, targ, reduce=False):\n",
    "    # operating on last dimension and assuming 2D matrices\n",
    "    x = x - torch.max(x, dim=-1, keepdim=True)[0] # shift by max\n",
    "    x = x - torch.log(torch.exp(x).sum(dim=-1, keepdim=True)) # instead of division in exp space\n",
    "    l = -1.0 * x[torch.arange(x.shape[0]), targ] # loss is taken from the correct class index\n",
    "\n",
    "    if reduce:\n",
    "        return l.mean()\n",
    "        \n",
    "    return l\n",
    "\n",
    "# From karpathy\n",
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "    \n",
    "\n",
    "nclasses = 5\n",
    "seq_len = 10\n",
    "examples = 3\n",
    "\n",
    "g = torch.Generator().manual_seed(123)\n",
    "inp = torch.rand((examples,seq_len, nclasses), dtype=torch.float64, generator=g)\n",
    "targ = torch.randint(0,nclasses, size=(examples,seq_len), dtype=torch.long, generator=g)\n",
    "\n",
    "inp = inp.view(-1, nclasses)\n",
    "targ = targ.view(-1)\n",
    "\n",
    "# check SM \n",
    "assert torch.allclose(softmax(inp), F.softmax(inp, dim=-1))\n",
    "\n",
    "# Check CE\n",
    "assert torch.allclose(F.cross_entropy(inp, targ, reduction='none'), cross_entropy(softmax(inp), targ, reduce=False))\n",
    "\n",
    "# Will fial\n",
    "# Check for numerical stability (scaling input will results in inf log(very small number) == -inf\n",
    "inp *= 1000\n",
    "\n",
    "assert torch.allclose(softmax(inp), F.softmax(inp, dim=-1))\n",
    "torch.allclose(F.cross_entropy(inp, targ, reduction='none'), cross_entropy(softmax(inp), targ, reduce=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "319220fb-c74d-4d92-b8aa-6563d0a76d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert torch.allclose(log_softmax(inp), F.log_softmax(inp, dim=-1))\n",
    "assert torch.allclose(F.cross_entropy(inp, targ, reduction='none'), cross_entropy_of_log(log_softmax(inp), targ, reduce=False))\n",
    "\n",
    "# check combined version\n",
    "assert torch.allclose(F.cross_entropy(inp, targ, reduction='none'), cross_entropy_logits(inp, targ, reduce=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "ba6c9346-20da-4aab-8688-6c63c111d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, size, seq_len, vocab):\n",
    "        assert vocab < 2**16\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.size = size\n",
    "        self.data = torch.randint(low=0, high=vocab, size=(size * seq_len,), dtype=torch.int).view((size, seq_len))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return self.data[idx,:-1], self.data[idx,1:]\\\n",
    "        # Decided to return the whole sequence\n",
    "        return self.data[idx,:]\n",
    "\n",
    "\n",
    "class MYDS(Dataset):\n",
    "    import os\n",
    "    import urllib.request\n",
    "    \n",
    "    def __init__(self, path, max_length, stride):\n",
    "        max_length += 1\n",
    "        if not os.path.exists(path):\n",
    "            url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "                   \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "                   \"the-verdict.txt\")\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "            \n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.raw_text = f.read()\n",
    "            \n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        token_ids = self.tokenizer.encode(self.raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        self.data = []\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            #target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.data.append(torch.tensor(input_chunk))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return self.data[idx,:-1], self.data[idx,1:]\\\n",
    "        # Decided to return the whole sequence\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    \n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.nheads = config.nheads\n",
    "        self.hdim = config.hdim\n",
    "        # q, k, v combined in 1 matrix (to be split later).\n",
    "        # We assume input dimension is same as output dimension after projections\n",
    "        self.qkv = nn.parameter.Parameter(torch.rand(size=(config.hdim, 3 * config.nheads * config.hdim), dtype=torch.float32) / math.sqrt(config.hdim))\n",
    "        self.projection = nn.parameter.Parameter(torch.rand(size=(config.hdim * config.nheads, config.hdim), dtype=torch.float32) / math.sqrt(config.hdim))\n",
    "        # which cells to mask? the upper left by setting them to 0 (diagonal is not masked)\n",
    "        mask = torch.tril(torch.ones(config.seq_len,config.seq_len))\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.dropout = nn.Dropout(p=config.att_drop)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch x sequence x hdim)\n",
    "        B, S, H = x.size()\n",
    "        \n",
    "        # split into Q K V matrices. Each of shape [hdim, nheads * hdim]. \n",
    "        # alternatively: x =  x @ self.qkv\n",
    "        Q, K, V = torch.split(self.qkv, H * self.nheads, 1)\n",
    "\n",
    "        # project input into q,k,v (batch x nheads x seq x hdim)\n",
    "        q = (x @ Q).view(B,self.nheads,S,H)\n",
    "        k = (x @ K).view(B,self.nheads,S,H)\n",
    "        v = (x @ V).view(B,self.nheads,S,H)\n",
    "\n",
    "        # calculate dot product (batch x heads x seq(queries) x seq(keys))\n",
    "        att = torch.matmul(q, k.transpose(3,2)) * (1.0 / math.sqrt(self.hdim))\n",
    "\n",
    "        # Only difference of causal self-att.\n",
    "        # Causal masking (in place):  set all items to the right of index i,i to zero (-inf before softmax)\n",
    "        att.masked_fill_(torch.eq(self.mask.view(1,1,S,S), torch.scalar_tensor(0)), -torch.inf)\n",
    "\n",
    "        # Apply softmax then droptout\n",
    "        att = F.softmax(att, dim=-1) # same shape\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        out = att @ v # batch x heads x seq x hdim\n",
    "\n",
    "        # Project from all heads back to hdim\n",
    "        out = out.view(B,S,-1) # concat all heads: batch x seq x hdim*nheads\n",
    "        out = out @ self.projection # batch x seq x hdim\n",
    "        #print(out.mean())\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttBlock(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(config.hdim)\n",
    "        self.layer_norm2 = nn.LayerNorm(config.hdim)\n",
    "        \n",
    "        ffexpand = nn.Linear(config.hdim, 4 * config.hdim, bias=False)\n",
    "        gelu = NewGELU()\n",
    "        ffshrink = nn.Linear(4 * config.hdim, config.hdim, bias=False)\n",
    "        self.res_drop = nn.Dropout(config.res_drop)\n",
    "        \n",
    "        self.att = SelfAttention(config)\n",
    "        self.mlp = nn.Sequential(ffexpand, gelu, ffshrink, self.res_drop)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch x sequence)\n",
    "        #print(x.size())\n",
    "\n",
    "        out = self.att(self.layer_norm1(x))\n",
    "        out = self.res_drop(out)\n",
    "        out = self.layer_norm2(x + out)\n",
    "        out = self.mlp(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# todo: positional encoding\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, config, layers=3):\n",
    "        super().__init__()\n",
    "        self.vocab_size = config.vocab\n",
    "        self.emb = torch.nn.Embedding(config.vocab, config.hdim)\n",
    "        self.emb_drop = nn.Dropout(config.emb_drop)\n",
    "        self.layers = nn.Sequential(*[AttBlock(config) for _ in range(config.layers)])\n",
    "        self.last_layer_norm = nn.LayerNorm(config.hdim)\n",
    "        self.llm_head = nn.parameter.Parameter(torch.rand(size=(config.hdim, config.vocab), dtype=torch.float32) / math.sqrt(config.vocab))\n",
    "        self.loss = cross_entropy_logits\n",
    "        print(f\"Total parameters: {sum(p.numel() for p in self.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = self.emb(x[:,:-1])\n",
    "        outs = self.emb_drop(outs)\n",
    "        B, T, nh = outs.size()\n",
    "        outs = self.layers(outs)\n",
    "        outs = self.last_layer_norm(outs)\n",
    "        outs = outs @ self.llm_head # B x S X vocab_size\n",
    "        #print(f\"Output shape: {outs.shape}\")\n",
    "        loss = self.loss(outs.view(-1,self.vocab_size), x[:,1:].contiguous().view(-1), reduce=True)\n",
    "        return loss, torch.argmax(outs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "d4e34198-87bb-4911-af52-705ecbffc5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 79002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor(5.3012, grad_fn=<MeanBackward0>),\n",
       "  tensor([[ 62,  55,  73, 193,  57,  96,  73,  73, 163, 193],\n",
       "          [ 50, 193, 167,  73,  73, 174,  96,  12,  13,  11],\n",
       "          [ 96,  68, 102,  73,  73, 140, 193, 193,  96,  96],\n",
       "          [113,  96, 193,  96, 193, 193, 193, 163, 148,  99]])),\n",
       " tensor([[0, 2, 2, 0, 3, 4, 3, 3, 4, 0, 0],\n",
       "         [3, 0, 0, 3, 3, 3, 2, 2, 4, 1, 2],\n",
       "         [1, 4, 1, 4, 3, 2, 0, 3, 1, 3, 3],\n",
       "         [0, 2, 3, 4, 0, 0, 0, 4, 3, 0, 0]], dtype=torch.int32))"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "o = model(b)\n",
    "o, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "b1b86993-1eb2-4277-b558-2b340b226ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n",
      "Total parameters: 1618624\n"
     ]
    }
   ],
   "source": [
    "## data configs\n",
    "\n",
    "@dataclass\n",
    "class MyModelConfig:\n",
    "    data_size = 100\n",
    "    seq_len = 10  #actual context is seq_len\n",
    "    vocab = 5\n",
    "    batch_size = 8\n",
    "    epochs = 200\n",
    "    layers = 2\n",
    "    nheads = 3\n",
    "    hdim = 16\n",
    "    att_drop = .01\n",
    "    res_drop = .01\n",
    "    emb_drop = .01\n",
    "\n",
    "# Load config\n",
    "conf = MyModelConfig()\n",
    "\n",
    "# Load dataset, Add 1 to context len since we truncate\n",
    "#data = DummyDataset(conf.data_size, conf.seq_len + 1, conf.vocab)\n",
    "data = MYDS(\"verdict.txt\", conf.seq_len, stride=5)\n",
    "conf.vocab = data.tokenizer.n_vocab\n",
    "print(f\"Vocab size: {conf.vocab}\")\n",
    "#data.data[0]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(data, batch_size=conf.batch_size)\n",
    "\n",
    "\n",
    "model = MyModel(conf)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=0.001, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "7ece8705-8863-4b90-a702-198c89924baa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target modules {'q', 'v'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[762], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, TaskType\n\u001b[1;32m      5\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mSEQ_2_SEQ_LM, inference_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GDrive/workspace/repo/dl/venv/lib/python3.11/site-packages/peft/mapping.py:149\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    148\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GDrive/workspace/repo/dl/venv/lib/python3.11/site-packages/peft/peft_model.py:1599\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1599\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_encoder_decoder_kwargs_for_generation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1602\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39m_prepare_encoder_decoder_kwargs_for_generation\n\u001b[1;32m   1603\u001b[0m     )\n",
      "File \u001b[0;32m~/GDrive/workspace/repo/dl/venv/lib/python3.11/site-packages/peft/peft_model.py:138\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gradient_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/GDrive/workspace/repo/dl/venv/lib/python3.11/site-packages/peft/tuners/lora/model.py:139\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/GDrive/workspace/repo/dl/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:166\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/GDrive/workspace/repo/dl/venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:375\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key\u001b[38;5;241m=\u001b[39mkey)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m     )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# It's important to set the adapter here (again), because otherwise it can happen that if a 2nd adapter is\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# added, and it targets different layer(s) than the first adapter (which is active), then those different\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# layers will be activated, which we don't want.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_adapter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters)\n",
      "\u001b[0;31mValueError\u001b[0m: Target modules {'q', 'v'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1, get_modules=[\"q\", \"v\"])\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "1cd6a917-3986-495a-a9b2-889587ded2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n",
      "Total parameters: 1618624\n",
      "Epoch:2/step:200:  loss: 6.431022644042969\n",
      "Epoch:4/step:400:  loss: 6.165279388427734\n",
      "Epoch:5/step:600:  loss: 5.853367805480957\n",
      "Epoch:7/step:800:  loss: 6.129121780395508\n",
      "Epoch:8/step:1000:  loss: 5.976624488830566\n",
      "Epoch:10/step:1200:  loss: 5.519522190093994\n",
      "Epoch:11/step:1400:  loss: 4.595290184020996\n",
      "Epoch:13/step:1600:  loss: 4.9460248947143555\n",
      "Epoch:14/step:1800:  loss: 4.727682590484619\n",
      "Epoch:16/step:2000:  loss: 4.704626083374023\n",
      "Epoch:18/step:2200:  loss: 5.030416011810303\n",
      "Epoch:19/step:2400:  loss: 4.241650581359863\n",
      "Epoch:21/step:2600:  loss: 4.089865207672119\n",
      "Epoch:22/step:2800:  loss: 4.1062774658203125\n",
      "Epoch:24/step:3000:  loss: 3.857464551925659\n",
      "Epoch:25/step:3200:  loss: 4.203387260437012\n",
      "Epoch:27/step:3400:  loss: 3.8177058696746826\n",
      "Epoch:28/step:3600:  loss: 3.683701992034912\n",
      "Epoch:30/step:3800:  loss: 3.565894603729248\n",
      "Epoch:32/step:4000:  loss: 3.4561429023742676\n",
      "Epoch:33/step:4200:  loss: 3.396604061126709\n",
      "Epoch:35/step:4400:  loss: 3.3009681701660156\n",
      "Epoch:36/step:4600:  loss: 3.471595048904419\n",
      "Epoch:38/step:4800:  loss: 2.8972067832946777\n",
      "Epoch:39/step:5000:  loss: 2.8432767391204834\n",
      "Epoch:41/step:5200:  loss: 2.8263375759124756\n",
      "Epoch:42/step:5400:  loss: 3.1514577865600586\n",
      "Epoch:44/step:5600:  loss: 2.730769157409668\n",
      "Epoch:45/step:5800:  loss: 2.6031529903411865\n",
      "Epoch:47/step:6000:  loss: 2.504082441329956\n",
      "Epoch:49/step:6200:  loss: 2.5834224224090576\n",
      "Epoch:50/step:6400:  loss: 2.520646572113037\n",
      "Epoch:52/step:6600:  loss: 3.0363504886627197\n",
      "Epoch:53/step:6800:  loss: 2.4949560165405273\n",
      "Epoch:55/step:7000:  loss: 2.4903359413146973\n",
      "Epoch:56/step:7200:  loss: 2.5920491218566895\n",
      "Epoch:58/step:7400:  loss: 2.3746678829193115\n",
      "Epoch:59/step:7600:  loss: 2.7235567569732666\n",
      "Epoch:61/step:7800:  loss: 2.4363319873809814\n",
      "Epoch:63/step:8000:  loss: 2.225766181945801\n",
      "Epoch:64/step:8200:  loss: 1.9800422191619873\n",
      "Epoch:66/step:8400:  loss: 2.3802151679992676\n",
      "Epoch:67/step:8600:  loss: 2.5287413597106934\n",
      "Epoch:69/step:8800:  loss: 2.4496867656707764\n",
      "Epoch:70/step:9000:  loss: 2.180802822113037\n",
      "Epoch:72/step:9200:  loss: 2.2137153148651123\n",
      "Epoch:73/step:9400:  loss: 1.967359185218811\n",
      "Epoch:75/step:9600:  loss: 1.6784318685531616\n",
      "Epoch:76/step:9800:  loss: 1.9848963022232056\n",
      "Epoch:78/step:10000:  loss: 1.607757329940796\n",
      "Epoch:80/step:10200:  loss: 2.0371828079223633\n",
      "Epoch:81/step:10400:  loss: 1.7833807468414307\n",
      "Epoch:83/step:10600:  loss: 1.598706603050232\n",
      "Epoch:84/step:10800:  loss: 2.266186237335205\n",
      "Epoch:86/step:11000:  loss: 1.515277624130249\n",
      "Epoch:87/step:11200:  loss: 1.7461872100830078\n",
      "Epoch:89/step:11400:  loss: 2.050063371658325\n",
      "Epoch:90/step:11600:  loss: 2.6126837730407715\n",
      "Epoch:92/step:11800:  loss: 1.9217731952667236\n",
      "Epoch:94/step:12000:  loss: 1.5924932956695557\n",
      "Epoch:95/step:12200:  loss: 1.5845236778259277\n",
      "Epoch:97/step:12400:  loss: 1.922540307044983\n",
      "Epoch:98/step:12600:  loss: 1.4742577075958252\n",
      "Epoch:100/step:12800:  loss: 1.9396374225616455\n",
      "Epoch:101/step:13000:  loss: 1.8753535747528076\n",
      "Epoch:103/step:13200:  loss: 1.2975268363952637\n",
      "Epoch:104/step:13400:  loss: 1.9270890951156616\n",
      "Epoch:106/step:13600:  loss: 1.3480348587036133\n",
      "Epoch:107/step:13800:  loss: 1.7184845209121704\n",
      "Epoch:109/step:14000:  loss: 1.4592535495758057\n",
      "Epoch:111/step:14200:  loss: 1.5034804344177246\n",
      "Epoch:112/step:14400:  loss: 2.0420312881469727\n",
      "Epoch:114/step:14600:  loss: 1.4736398458480835\n",
      "Epoch:115/step:14800:  loss: 2.0695443153381348\n",
      "Epoch:117/step:15000:  loss: 1.1573203802108765\n",
      "Epoch:118/step:15200:  loss: 1.531669020652771\n",
      "Epoch:120/step:15400:  loss: 1.2326900959014893\n",
      "Epoch:121/step:15600:  loss: 1.6632912158966064\n",
      "Epoch:123/step:15800:  loss: 1.3561122417449951\n",
      "Epoch:125/step:16000:  loss: 1.702275037765503\n",
      "Epoch:126/step:16200:  loss: 1.221563458442688\n",
      "Epoch:128/step:16400:  loss: 1.378413438796997\n",
      "Epoch:129/step:16600:  loss: 1.4527946710586548\n",
      "Epoch:131/step:16800:  loss: 1.328400731086731\n",
      "Epoch:132/step:17000:  loss: 1.6244287490844727\n",
      "Epoch:134/step:17200:  loss: 1.282557725906372\n",
      "Epoch:135/step:17400:  loss: 1.2534842491149902\n",
      "Epoch:137/step:17600:  loss: 1.5926624536514282\n",
      "Epoch:138/step:17800:  loss: 1.3928402662277222\n",
      "Epoch:140/step:18000:  loss: 1.6644699573516846\n",
      "Epoch:142/step:18200:  loss: 1.5917984247207642\n",
      "Epoch:143/step:18400:  loss: 1.5676403045654297\n",
      "Epoch:145/step:18600:  loss: 1.341119647026062\n",
      "Epoch:146/step:18800:  loss: 1.3999288082122803\n",
      "Epoch:148/step:19000:  loss: 1.204521894454956\n",
      "Epoch:149/step:19200:  loss: 1.3358718156814575\n",
      "Epoch:151/step:19400:  loss: 1.11642587184906\n",
      "Epoch:152/step:19600:  loss: 1.7074079513549805\n",
      "Epoch:154/step:19800:  loss: 1.4930615425109863\n",
      "Epoch:156/step:20000:  loss: 1.1263248920440674\n",
      "Epoch:157/step:20200:  loss: 1.5443447828292847\n",
      "Epoch:159/step:20400:  loss: 1.5017945766448975\n",
      "Epoch:160/step:20600:  loss: 0.9009550213813782\n",
      "Epoch:162/step:20800:  loss: 1.415405511856079\n",
      "Epoch:163/step:21000:  loss: 1.3389346599578857\n",
      "Epoch:165/step:21200:  loss: 1.0278141498565674\n",
      "Epoch:166/step:21400:  loss: 1.7227329015731812\n",
      "Epoch:168/step:21600:  loss: 1.624328374862671\n",
      "Epoch:169/step:21800:  loss: 1.5970538854599\n",
      "Epoch:171/step:22000:  loss: 1.102632761001587\n",
      "Epoch:173/step:22200:  loss: 1.501668930053711\n",
      "Epoch:174/step:22400:  loss: 1.7374498844146729\n",
      "Epoch:176/step:22600:  loss: 0.8315871953964233\n",
      "Epoch:177/step:22800:  loss: 1.5564135313034058\n",
      "Epoch:179/step:23000:  loss: 1.0037199258804321\n",
      "Epoch:180/step:23200:  loss: 1.259552240371704\n",
      "Epoch:182/step:23400:  loss: 0.7644103169441223\n",
      "Epoch:183/step:23600:  loss: 1.7274280786514282\n",
      "Epoch:185/step:23800:  loss: 1.9007165431976318\n",
      "Epoch:187/step:24000:  loss: 1.2022539377212524\n",
      "Epoch:188/step:24200:  loss: 1.3133649826049805\n",
      "Epoch:190/step:24400:  loss: 1.1188185214996338\n",
      "Epoch:191/step:24600:  loss: 1.403070092201233\n",
      "Epoch:193/step:24800:  loss: 1.1809428930282593\n",
      "Epoch:194/step:25000:  loss: 1.4015142917633057\n",
      "Epoch:196/step:25200:  loss: 0.6627935171127319\n",
      "Epoch:197/step:25400:  loss: 1.0247586965560913\n",
      "Epoch:199/step:25600:  loss: 0.8219085931777954\n",
      "Epoch:200/step:25800:  loss: 0.4346376061439514\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "tsteps = -1\n",
    "lsteps = 200\n",
    "iters = 0\n",
    "#for e in tqdm.tqdm(range(epochs)):\n",
    "for i, e in enumerate(range(conf.epochs)):\n",
    "    for b in train_loader:\n",
    "        iters += 1\n",
    "        model.zero_grad()\n",
    "        loss, outs = model(b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iters % lsteps == 0:\n",
    "            print(f\"Epoch:{i+1}/step:{iters}:  loss: {loss.item()}\")\n",
    "        if tsteps> -1 and iters > tsteps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "f86d3d81-d793-4b42-b2c5-248117a672f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  329,   502,     0,   383,   520,  5493,    82,  1302,  3436,    11,\n",
       "            290],\n",
       "         [ 5493,    82,  1302,  3436,    11,   290,  1645,  1752,   438,  4360,\n",
       "            612],\n",
       "         [  290,  1645,  1752,   438,  4360,   612,   338,   645, 42393,   803,\n",
       "            674]]),\n",
       " tensor([[  502,     0,   632,   520,  5493,    82,  1302,  3436,    11,   290],\n",
       "         [   82,  1302,  3436,    11,   290,  1645,  1752,   438,  4360,   326],\n",
       "         [ 1645,  1752,   438,  4360,   612,   338,   645, 42393,   803,   674]]))"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "f86f918a-3af1-4f8b-94cb-c5df4a107d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1808, 15632,   438,  2016,   257,   922,  5891,  1576,   438,   568]])"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data[2].view(1,-1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "0e0fd451-400c-4908-abcd-ced9b86af06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" picture 'way up; but I don't think of\"]\n",
      "[\"--way up, and I looked't dab of\"]\n"
     ]
    }
   ],
   "source": [
    "idx = 26\n",
    "print(data.tokenizer.decode_batch(data[idx].view(1,-1).numpy()))\n",
    "model.eval()\n",
    "print(data.tokenizer.decode_batch(model(data[idx].view(1,-1))[1].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "9db61f9b-cf03-441c-bb7f-13940c3ff5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sum([t.numel() for t in optimizer.param_groups[0]['params']]) / sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "9e4f42a0-9c6c-4b26-8e07-8c147926dcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
